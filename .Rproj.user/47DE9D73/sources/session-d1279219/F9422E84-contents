# Set your dataset path
base_path <- "./../archive/nepali_news_dataset_20_categories_large/nepali_news_dataset_20_categories_large"


# Source the needed files
source("nepali-news-clustering.R")
source("nepali-news-memory-efficient.R")
source("nepali-stemming.R")
source("nepali-news-stemming-example.R")
source("nepali-alluvial-plot.R")  # Optional for visualization


# Load the data if not already loaded
if (!exists("news_data")) {
  news_data <- read_nepali_news(base_path)
}

# Process with stemming
results <- process_news_data_with_stemming(news_data)

# Analyze results
stem_analysis <- analyze_clusters_with_stems(results)

alluvial_plot <- create_alluvial_plot(news_data, results$full_clusters)



source("next-steps-clustering.R")
related_news <- find_related_news(news_data)




# 1. Preprocess with improved stopword removal and stemming
news_data$processed_text <- sapply(news_data$text, preprocess_nepali_with_stemming)

# 2. Create DTM and apply feature selection
dtm <- DocumentTermMatrix(Corpus(VectorSource(news_data$processed_text)))
filtered_dtm <- improved_feature_selection(dtm)

# 3. Create TF-IDF
tfidf <- weightTfIdf(filtered_dtm)

# 4. Reduce dimensions
reduced_data <- reduce_dimensions(tfidf, n_components = 20)

# 5. Find optimal number of clusters
optimal_k <- find_optimal_k_improved(reduced_data$reduced_matrix, max_k = 30, method = "silhouette")

# 6. Perform clustering with optimal k
clusters <- perform_kmeans_on_reduced(reduced_data$reduced_matrix, optimal_k)

# 7. Map clusters back to full dataset
full_clusters <- map_clusters_to_full_dataset(
  clusters$cluster,
  reduced_data$non_empty_docs,
  nrow(news_data)
)

# 8. Create visualization of category-cluster relationships
river <- create_riverplot(news_data, full_clusters, save_file = "nepali_news_riverplot.pdf")

# 9. Analyze clusters
stem_analysis <- analyze_clusters_with_stems(list(
  news_data = news_data,
  dtm = filtered_dtm,
  full_clusters = full_clusters
))






# Example workflow for the complete analysis pipeline:
# 
# # Step 1: Process news data with improved stemming
# results <- process_news_data_with_stemming(news_data)
# 
# # Step 2: Perform basic clustering analysis
# stem_analysis <- analyze_clusters_with_stems(results)
# 
# # Step 3: Find related news articles
# related_news <- find_related_news(news_data)
# 
# # Step 4: Compare coverage across categories
# coverage_stats <- compare_news_coverage(related_news)
# 
# # Step 5: Identify potential blindspots
# # (This would require a list of sources categorized by political leaning)
# left_sources <- c("source1", "source2", "source3")
# right_sources <- c("source4", "source5", "source6")
# blindspots <- identify_blindspots(coverage_stats, left_sources, right_sources)




